@namespace ActualChat.Audio.UI.Blazor.Components
@implements IAudioRecorderBackend
@using ActualChat.Audio.UI.Blazor.Module
@implements IAsyncDisposable
@inject Session _session
@inject IJSRuntime _js
@inject ISourceAudioRecorder _sourceAudioRecorder

@{
    var cls = "";
    if (IsRecording)
        cls += "recording";
    else {
        cls += "not-recording";
    }
}

<button @onclick="ToggleRecording" class="@(_componentCssClass ??= this.ComponentCssClass())">
    <svg class="items-center fill-current @cls" xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="-2 -2 28 28">
        <path d="M16 11c0 2.209-1.791 4-4 4s-4-1.791-4-4v-7c0-2.209 1.791-4 4-4s4 1.791 4 4v7zm4-2v2c0 4.418-3.582 8-8 8s-8-3.582-8-8v-2h2v2c0 3.309 2.691 6 6 6s6-2.691 6-6v-2h2zm-7 13v-2h-2v2h-4v2h10v-2h-4z"/>
    </svg>
</button>

@code {
    [Inject] private ILogger<AudioRecorder> Log { get; init; } = null!;
    private ILogger? DebugLog => DebugMode ? Log : null;
    private bool DebugMode { get; } = Constants.DebugMode.AudioRecording;
    private static string? _componentCssClass;

    private IJSObjectReference? JSRef { get; set; }
    private DotNetObjectReference<IAudioRecorderBackend> BlazorRef { get; set; } = null!;
    private Channel<BlobPart>? AudioChannel { get; set; }
    private int Index { get; set; }
    private uint LastSeqNum { get; set; } = 0;
    private bool IsRecording => AudioChannel != null;

    [Parameter]
    public string ChatId { get; set; } = "";

    public async ValueTask DisposeAsync() {
        await StopRecording().ConfigureAwait(true);
        await JSRef.DisposeSilentlyAsync().ConfigureAwait(true);
        BlazorRef?.Dispose();
    }

    protected override async Task OnAfterRenderAsync(bool firstRender) {
        if (firstRender) {
            BlazorRef = DotNetObjectReference.Create<IAudioRecorderBackend>(this);
            JSRef = await _js.InvokeAsync<IJSObjectReference>(
                $"{AudioBlazorUIModule.ImportName}.AudioRecorder.create",
                BlazorRef, DebugMode
                ).ConfigureAwait(true);
        }
        DebugLog?.LogDebug(
            "OnAfterRender: Chat #{ChatId}, firstRender = {FirstRender}",
            ChatId, firstRender);
    }

    private Task ToggleRecording()
        => IsRecording ? StopRecording() : StartRecording();

    private async Task StartRecording() {
        if (AudioChannel != null)
            return;

        AudioChannel = Channel.CreateBounded<BlobPart>(
            new BoundedChannelOptions(128) {
                SingleWriter = true,
                SingleReader = true,
                AllowSynchronousContinuations = true,
            });
        StateHasChanged();
        if (JSRef != null)
            await JSRef.InvokeVoidAsync("startRecording");
        DebugLog?.LogDebug("StartRecording");
    }

    private async Task StopRecording() {
        DebugLog?.LogDebug("StopRecording, current AudioChannel status is {AudioChannelStatus}",
            AudioChannel == null ? "closed" : "opened"
        );

        if (AudioChannel == null)
            return;
        try {
            if (JSRef != null)
                await JSRef.InvokeVoidAsync("stopRecording");
        }
        finally {
            if (AudioChannel != null) {
                AudioChannel.Writer.Complete();
                AudioChannel = null;
                StateHasChanged();
            }
        }
    }

    // JS backend callback handlers

    [JSInvokable]
    public void OnStartRecording() {
        if (AudioChannel == null)
            return;

        var audioFormat = new AudioFormat {
            CodecKind = AudioCodecKind.Opus,
            ChannelCount = 1,
            SampleRate = 48_000,
        };
        var clientStartOffset = CpuClock.Now.EpochOffset.TotalSeconds;
        var audioRecord = new AudioRecord(ChatId, audioFormat, "RU-ru", clientStartOffset);
        var audioStream = AudioChannel.Reader.ReadAllAsync(CancellationToken.None);
        _ = BackgroundTask.Run(
            () => _sourceAudioRecorder.RecordSourceAudio(_session, audioRecord, audioStream, CancellationToken.None),
            Log, $"{nameof(_sourceAudioRecorder.RecordSourceAudio)} failed");
    }

    [JSInvokable]
    public async Task OnAudioData(byte[] chunk) {
        if (AudioChannel == null)
            return;

        var seqNum = Unsafe.As<byte, uint>(ref chunk[0]);

        if (seqNum != 0 && (seqNum - 1) != LastSeqNum)
            Log.LogError("OnAudioData: SeqNum mismatch, current = {SeqNum}, last: {LastSeqNum}", seqNum, LastSeqNum);

        LastSeqNum = seqNum;
        var blobPart = new BlobPart(Index++, chunk[4..]);
        DebugLog?.LogDebug("OnAudioData: got packet #{SeqNum}: {BlobPart}", seqNum, blobPart);
        await AudioChannel.Writer.WriteAsync(blobPart);
    }

    [JSInvokable]
    public void OnRecordingStopped() {
        DebugLog?.LogDebug("OnRecordingStopped: current AudioChannel status is {AudioChannelStatus}",
            AudioChannel == null ? "closed" : "opened"
        );
        // Does the same as StopRecording; we assume here that recording
        // might be recognized as stopped by JS backend as well
        if (AudioChannel == null)
            return;

        AudioChannel.Writer.Complete();
        AudioChannel = null;
        StateHasChanged();
    }
}
