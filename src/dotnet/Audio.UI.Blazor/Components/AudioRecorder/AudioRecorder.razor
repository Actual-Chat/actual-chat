@namespace ActualChat.Audio.UI.Blazor.Components
@implements IAudioRecorderBackend
@using ActualChat.Audio.UI.Blazor.Module
@using ActualChat.Media
@using System.Buffers.Binary
@implements IAsyncDisposable

@{
    var cls = "";
    if (IsRecording)
        cls += "recording";
    else {
        cls += "not-recording";
    }
}

<button class="@(_class ??= this.DefaultClass()) p-1 transition-all duration-150"
        @onclick="ToggleRecording" >
    <svg class="fill-current w-8 h-8 @cls" xmlns="http://www.w3.org/2000/svg" viewBox="-1 -1 26 26">
        <path d="M13 18.94V21h3a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h3v-2.06A8 8 0 0 1 4 11a1 1 0 0 1 2 0 6 6 0 1 0 12 0 1 1 0 0 1 2 0 8 8 0 0 1-7 7.94zM12 1a4 4 0 0 1 4 4v6a4 4 0 1 1-8 0V5a4 4 0 0 1 4-4zm0 2a2 2 0 0 0-2 2v6a2 2 0 1 0 4 0V5a2 2 0 0 0-2-2z"/>
    </svg>
</button>

@code {
    private static string? _class;

    [Inject] private ILogger<AudioRecorder> Log { get; init; } = null!;
    private ILogger? DebugLog => DebugMode ? Log : null;
    private bool DebugMode => Constants.DebugMode.AudioRecording;

    [Inject] private Session Session { get; init; } = null!;
    [Inject] private IJSRuntime JS { get; init; } = null!;
    [Inject] private ISourceAudioRecorder SourceAudioRecorder { get; init; } = null!;

    private IJSObjectReference? JSRef { get; set; }
    private DotNetObjectReference<IAudioRecorderBackend> BlazorRef { get; set; } = null!;
    private Channel<RecordingPart> AudioChannel { get; set; } = null!;
    private int Index { get; set; }
    private bool IsRecording => AudioChannel != null!;

    [Parameter]
    public string ChatId { get; set; } = "";

    public async ValueTask DisposeAsync() {
        await StopRecording().ConfigureAwait(true);
        await JSRef.DisposeSilentlyAsync().ConfigureAwait(true);
        // ReSharper disable once ConstantConditionalAccessQualifier
        BlazorRef?.Dispose();
    }

    protected override async Task OnAfterRenderAsync(bool firstRender) {
        if (firstRender) {
            BlazorRef = DotNetObjectReference.Create<IAudioRecorderBackend>(this);
            JSRef = await JS.InvokeAsync<IJSObjectReference>(
                $"{AudioBlazorUIModule.ImportName}.AudioRecorder.create",
                BlazorRef, DebugMode
                ).ConfigureAwait(true);
        }
    }

    private Task ToggleRecording()
        => IsRecording ? StopRecording() : StartRecording();

    private async Task StartRecording() {
        if (IsRecording) return;
        DebugLog?.LogDebug(nameof(StartRecording));

        AudioChannel = Channel.CreateBounded<RecordingPart>(
            new BoundedChannelOptions(128) {
                SingleWriter = true,
                SingleReader = true,
                AllowSynchronousContinuations = true,
            });
        StateHasChanged();
        if (JSRef != null)
            await JSRef.InvokeVoidAsync("startRecording");
    }

    private async Task StopRecording() {
        if (!IsRecording) return;
        DebugLog?.LogDebug(nameof(StopRecording));

        var audioChannel = AudioChannel;
        _ = Task.Run(async () => {
            await Task.Delay(TimeSpan.FromSeconds(5)).ConfigureAwait(true);
            if (AudioChannel != audioChannel)
                return; // We don't want to stop the next recording here :)

            Log.LogWarning(nameof(OnRecordingStopped) + " wasn't invoked on time by JS backend");
            OnRecordingStopped();
        });
        if (JSRef != null)
            await JSRef.InvokeVoidAsync("stopRecording");
    }

    // JS backend callback handlers

    [JSInvokable]
    public void OnStartRecording() {
        if (!IsRecording) return;
        DebugLog?.LogDebug(nameof(OnStartRecording));

        var audioFormat = new AudioFormat {
            CodecKind = AudioCodecKind.Opus,
            ChannelCount = 1,
            SampleRate = 48_000,
        };
        var clientStartOffset = CpuClock.Now.EpochOffset.TotalSeconds;
        var audioRecord = new AudioRecord(Session.Id.Value, ChatId, audioFormat, clientStartOffset);
        var audioStream = AudioChannel.Reader.ReadAllAsync(CancellationToken.None);
        _ = BackgroundTask.Run(
            () => SourceAudioRecorder.RecordSourceAudio(Session, audioRecord, audioStream, CancellationToken.None),
            Log, $"{nameof(SourceAudioRecorder.RecordSourceAudio)} failed");
    }

    [JSInvokable]
    public async Task OnAudioEventChunk(byte[] chunk) {
        if (!IsRecording) return;
        // DebugLog?.LogDebug(nameof(OnAudioEventChunk) + ": got {ChunkLength} byte(s)", chunk.Length);

        foreach (var part in DeserializeRecordingData(chunk))
            await AudioChannel.Writer.WriteAsync(part);
    }

    [JSInvokable]
    public void OnRecordingStopped() {
        // Does the same as StopRecording; we assume here that recording
        // might be recognized as stopped by JS backend as well
        if (!IsRecording) return;
        DebugLog?.LogDebug(nameof(OnRecordingStopped));

        AudioChannel.Writer.Complete();
        AudioChannel = null!;
        StateHasChanged();
    }

    private IEnumerable<RecordingPart> DeserializeRecordingData(byte[] data) {
        var offset = 0;
        while (offset < data.Length) {
            var type = (RecordingEventType)data[offset++];
            var length = BinaryPrimitives.ReadUInt16LittleEndian(data[offset..]);
            offset += 2;
            var startData = offset;
            offset += length;
            yield return type switch {
                RecordingEventType.Data => new(RecordingEventKind.Data) {
                    Data = data[startData..(startData + length)].ToArray(),
                },
                RecordingEventType.Pause => new(RecordingEventKind.Pause) {
                    RecordedAt = new Moment(BinaryPrimitives.ReadInt64LittleEndian(data[startData..])),
                    Offset = TimeSpan.FromSeconds(BinaryPrimitives.ReadSingleLittleEndian(data[(startData + 8)..])),
                },
                RecordingEventType.Resume => new(RecordingEventKind.Resume) {
                    RecordedAt = new Moment(BinaryPrimitives.ReadInt64LittleEndian(data[startData..])),
                    Offset = TimeSpan.FromSeconds(BinaryPrimitives.ReadSingleLittleEndian(data[(startData + 8)..])),
                },
                _ => throw new NotSupportedException($"Unsupported {nameof(RecordingEventKind)}: {type}."),
            };
        }
    }
}

