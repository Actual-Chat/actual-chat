@namespace ActualChat.Audio.UI.Blazor.Components
@implements IAudioRecorderBackend
@using ActualChat.Audio.UI.Blazor.Module
@using ActualChat.Media
@using System.Buffers.Binary
@implements IAsyncDisposable
@inject Session _session
@inject IJSRuntime _js
@inject ISourceAudioRecorder _sourceAudioRecorder

@{
    var cls = "";
    if (IsRecording)
        cls += "recording";
    else {
        cls += "not-recording";
    }
}

<button class="@(_class ??= this.DefaultClass()) align-top py-2 pr-3 outline-none rounded-r-full bg-transparent max-h-12 transition-all duration-150"
        @onclick="ToggleRecording" >
    <svg class="items-center fill-current @cls" xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="-2 -2 28 28">
        <path d="M16 11c0 2.209-1.791 4-4 4s-4-1.791-4-4v-7c0-2.209 1.791-4 4-4s4 1.791 4 4v7zm4-2v2c0 4.418-3.582 8-8 8s-8-3.582-8-8v-2h2v2c0 3.309 2.691 6 6 6s6-2.691 6-6v-2h2zm-7 13v-2h-2v2h-4v2h10v-2h-4z"/>
    </svg>
</button>

@code {
    private static string? _class;

    [Inject] private ILogger<AudioRecorder> Log { get; init; } = null!;
    private ILogger? DebugLog => DebugMode ? Log : null;
    private bool DebugMode => Constants.DebugMode.AudioRecording;

    private IJSObjectReference? JSRef { get; set; }
    private DotNetObjectReference<IAudioRecorderBackend> BlazorRef { get; set; } = null!;
    private Channel<RecordingPart> AudioChannel { get; set; } = null!;
    private int Index { get; set; }
    private bool IsRecording => AudioChannel != null!;

    [Parameter]
    public string ChatId { get; set; } = "";
    [Parameter]
    public string Language { get; set; } = "ru-RU";

    public async ValueTask DisposeAsync() {
        await StopRecording().ConfigureAwait(true);
        await JSRef.DisposeSilentlyAsync().ConfigureAwait(true);
        // ReSharper disable once ConstantConditionalAccessQualifier
        BlazorRef?.Dispose();
    }

    protected override async Task OnAfterRenderAsync(bool firstRender) {
        if (firstRender) {
            BlazorRef = DotNetObjectReference.Create<IAudioRecorderBackend>(this);
            JSRef = await _js.InvokeAsync<IJSObjectReference>(
                $"{AudioBlazorUIModule.ImportName}.AudioRecorder.create",
                BlazorRef, DebugMode
                ).ConfigureAwait(true);
        }
    }

    private Task ToggleRecording()
        => IsRecording ? StopRecording() : StartRecording();

    private async Task StartRecording() {
        if (IsRecording) return;
        DebugLog?.LogDebug(nameof(StartRecording));

        AudioChannel = Channel.CreateBounded<RecordingPart>(
            new BoundedChannelOptions(128) {
                SingleWriter = true,
                SingleReader = true,
                AllowSynchronousContinuations = true,
            });
        StateHasChanged();
        if (JSRef != null)
            await JSRef.InvokeVoidAsync("startRecording");
    }

    private async Task StopRecording() {
        if (!IsRecording) return;
        DebugLog?.LogDebug(nameof(StopRecording));

        var audioChannel = AudioChannel;
        _ = Task.Run(async () => {
            await Task.Delay(TimeSpan.FromSeconds(5)).ConfigureAwait(true);
            if (AudioChannel != audioChannel)
                return; // We don't want to stop the next recording here :)

            Log.LogWarning(nameof(OnRecordingStopped) + " wasn't invoked on time by JS backend");
            OnRecordingStopped();
        });
        if (JSRef != null)
            await JSRef.InvokeVoidAsync("stopRecording");
    }

    // JS backend callback handlers

    [JSInvokable]
    public void OnStartRecording() {
        if (!IsRecording) return;
        DebugLog?.LogDebug(nameof(OnStartRecording));

        var audioFormat = new AudioFormat {
            CodecKind = AudioCodecKind.Opus,
            ChannelCount = 1,
            SampleRate = 48_000,
        };
        var clientStartOffset = CpuClock.Now.EpochOffset.TotalSeconds;
        var audioRecord = new AudioRecord(ChatId, audioFormat, Language, clientStartOffset);
        var audioStream = AudioChannel.Reader.ReadAllAsync(CancellationToken.None);
        _ = BackgroundTask.Run(
            () => _sourceAudioRecorder.RecordSourceAudio(_session, audioRecord, audioStream, CancellationToken.None),
            Log, $"{nameof(_sourceAudioRecorder.RecordSourceAudio)} failed");
    }

    [JSInvokable]
    public async Task OnAudioEventChunk(byte[] chunk) {
        if (!IsRecording) return;
        // DebugLog?.LogDebug(nameof(OnAudioEventChunk) + ": got {ChunkLength} byte(s)", chunk.Length);

        foreach (var part in DeserializeRecordingData(chunk))
            await AudioChannel.Writer.WriteAsync(part);
    }

    [JSInvokable]
    public void OnRecordingStopped() {
        // Does the same as StopRecording; we assume here that recording
        // might be recognized as stopped by JS backend as well
        if (!IsRecording) return;
        DebugLog?.LogDebug(nameof(OnRecordingStopped));

        AudioChannel.Writer.Complete();
        AudioChannel = null!;
        StateHasChanged();
    }

    private IEnumerable<RecordingPart> DeserializeRecordingData(byte[] data) {
        var offset = 0;
        while (offset < data.Length) {
            var type = (RecordingEventType)data[offset++];
            var length = BinaryPrimitives.ReadUInt16LittleEndian(data[offset..]);
            offset += 2;
            var startData = offset;
            offset += length;
            yield return type switch {
                RecordingEventType.Data => new(RecordingEventKind.Data) {
                    Data = data[startData..(startData + length)].ToArray(),
                },
                RecordingEventType.Pause => new(RecordingEventKind.Pause) {
                    RecordedAt = new Moment(BinaryPrimitives.ReadInt64LittleEndian(data[startData..])),
                    Offset = TimeSpan.FromSeconds(BinaryPrimitives.ReadSingleLittleEndian(data[(startData + 8)..])),
                },
                RecordingEventType.Resume => new(RecordingEventKind.Resume) {
                    RecordedAt = new Moment(BinaryPrimitives.ReadInt64LittleEndian(data[startData..])),
                    Offset = TimeSpan.FromSeconds(BinaryPrimitives.ReadSingleLittleEndian(data[(startData + 8)..])),
                },
                _ => throw new NotSupportedException($"Unsupported {nameof(RecordingEventKind)}: {type}."),
            };
        }
    }
}

